{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Dataset consists of top 1000 rated movies\n",
    "#### Columns Description:\n",
    "##### 1) Title: describe the title of movie\n",
    "##### 2) Certificate: describe the audience of this movie is for \n",
    "##### 3) Duration: describe the duration of each movie in minutes\n",
    "##### 4) Genre: describe the movie type (action, drama ...etc.)\n",
    "##### 5) Rate: describe the rating of the movie out of 10\n",
    "##### 6) Metascore: describe the metascore of each movie\n",
    "##### 7) Description: describe brief info about the movie\n",
    "##### 8) Cast: describe the movie director and the lead actors\n",
    "##### 9) Info: describe more info such as number of people voted and the movie gross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm import tqdm\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# to stop showing warnings \n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load  Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(\"IMDB top 1000.csv\") # read dataset\n",
    "movies.info() # show brief info about dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "movies = movies.drop(['Unnamed: 0','Metascore', 'Cast', 'Info'], axis=1)\n",
    "\n",
    "# Use Simple Imputer to substitute null values in Certificate Column with 'Not Rated' \n",
    "Imputer = SimpleImputer(strategy='constant', fill_value='Not Rated')\n",
    "movies['Certificate'] = Imputer.fit_transform(movies['Certificate'].values.reshape(-1,1))\n",
    "\n",
    "movies.info() # show brief info about the dataset after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess supervised learning data frame\n",
    "\n",
    "# select Description and Genre columns for supervised learning dataframe\n",
    "df_supervised = movies[['Description','Genre']]\n",
    "\n",
    "# Create new column for each genre in Genre column (Drama, Action, ...etc.)\n",
    "df_supervised=df_supervised.join(pd.Series(df_supervised['Genre']).str.get_dummies(', '))\n",
    "\n",
    "# drop Genre column\n",
    "df_supervised=df_supervised.drop('Genre', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess unsupervised learning data frame\n",
    "\n",
    "# select Certificate, Duration, Rate, and Genre columns for unsupervised learning dataframe\n",
    "df_unsupervised = movies[['Certificate','Duration','Rate','Genre']]\n",
    "\n",
    "# encode Certificate column with the help of label encoder\n",
    "df_unsupervised['Certificate'] = LabelEncoder().fit_transform(df_unsupervised['Certificate'])\n",
    "\n",
    "# preprocess Duration column to get only numbers then change its type to integer\n",
    "df_unsupervised['Duration'] = [re.sub(\"[^0-9]\", \"\", x) for x in df_unsupervised['Duration']]\n",
    "df_unsupervised['Duration'] = df_unsupervised['Duration'].astype('int64')\n",
    "\n",
    "# Create new column for each genre in Genre column (Drama, Action, ...etc.)\n",
    "df_unsupervised=df_unsupervised.join(pd.Series(df_unsupervised['Genre']).str.get_dummies(', '))\n",
    "\n",
    "# drop Genre column\n",
    "df_unsupervised=df_unsupervised.drop('Genre', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration, Investigation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supervised.info() # show brief info about supervised learning dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_unsupervised.info() # show brief info about unsupervised learning dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 10 rated movies\n",
    "top_ranks = movies['Rate'].sort_values(ascending=False)[:10]\n",
    "\n",
    "# get the names of top 10 rated movies\n",
    "top_movies = movies.iloc[top_ranks.index,0].reset_index(drop=True)\n",
    "\n",
    "# clearn the names of top 10 rated movies\n",
    "top_movies = [re.sub(\"[^a-zA-z\\s]\", \"\", x).strip()  for x in top_movies]\n",
    "\n",
    "# show top 10 rated movies\n",
    "print(\"Top 10 Movies that have highest rating:\")\n",
    "for i, movie in enumerate(top_movies):\n",
    "    print(i+1,movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get genres names\n",
    "genres = df_supervised.columns[1:]\n",
    "\n",
    "# create genres movies counts list\n",
    "genres_counts = [df_supervised[genre].value_counts()[1] for genre in genres]\n",
    "\n",
    "# show bar plot Genres vs Count\n",
    "fig = plt.figure(figsize=(30,8))\n",
    "plt.bar(range(len(genres)),genres_counts, tick_label=genres)\n",
    "plt.title('Genres vs Count', fontsize=18)\n",
    "plt.ylabel('Count', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud for top 10 rated movies\n",
    "wordcloud = WordCloud(width = 1000, height = 800, random_state=21,\n",
    "                background_color ='white',  \n",
    "                min_font_size = 10).generate(' '.join(top_movies))\n",
    "\n",
    "# show wordcloud image\n",
    "fig = plt.figure(figsize=(25,6))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Top 10 Rated Movies', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principle Component Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data using standard scaler\n",
    "scaled_data = StandardScaler().fit_transform(df_unsupervised)\n",
    "\n",
    "# build an instance of PCA that keeps 3 principle components\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# get pca data by performing PCA on scaled data\n",
    "pca_data = pca.fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variance percentages\n",
    "variance_percentage = pca.explained_variance_ratio_*100\n",
    "\n",
    "# define principle components names\n",
    "PC_names = [('PC'+str(i+1)) for i in range(len(variance_percentage))]\n",
    "\n",
    "# show bar plot (variance percentage for each principle component)\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "plt.bar(range(len(variance_percentage)), variance_percentage, tick_label=PC_names)\n",
    "plt.xlabel('Principle Components', fontsize=16)\n",
    "plt.ylabel('Variance Percentage', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA 3D Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data after PCA as 3D scatter plot\n",
    "ax = plt.figure(figsize= (10,10)).gca(projection = '3d')\n",
    "ax.scatter(pca_data[:,0],pca_data[:,1],pca_data[:,2])\n",
    "ax.set(xlabel=' PC1',ylabel=' PC2',zlabel=' PC3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate supervised learning dataframe to descriptions and genres\n",
    "descriptions = df_supervised['Description']\n",
    "genres = df_supervised.drop('Description', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clean text function that takes a piece of text and return clean version of it \n",
    "def clean_text(text):\n",
    "    # separate text to sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    # loop through sentences\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = sentences[i].lower()  # make each sentence lower case\n",
    "        sentences[i] = re.sub(r'\\W', ' ', sentences[i])  # remove punctuations\n",
    "        sentences[i] = re.sub('[0-9]', '', sentences[i])  # remove numbers\n",
    "        sentences[i] = re.sub(r'\\s+', ' ', sentences[i])  # remove multiple white spaces\n",
    "\n",
    "    # load english stop words\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    # create clean text\n",
    "    clean_text = ''\n",
    "    # loop through sentences\n",
    "    for sentence in sentences:\n",
    "        # separate each sentence into words\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        # loop through the words\n",
    "        for word in words:\n",
    "            # if the word is not a stop word and it is not a single letter\n",
    "            if (word not in stop_words) and len(word) >= 2:\n",
    "                # add the word to clean text\n",
    "                clean_text += word + ' '\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean descriptions\n",
    "descriptions = descriptions.apply(clean_text)\n",
    "\n",
    "# transfrom descriptions to vectors where we represent each word using its count\n",
    "descriptions = CountVectorizer().fit_transform(descriptions).toarray()\n",
    "\n",
    "# for debugging purposes\n",
    "descriptions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X(features) = descriptions\n",
    "X = descriptions\n",
    "\n",
    "# create y(targets or labels) = genres\n",
    "y = genres.values\n",
    "\n",
    "# for debugging purposes\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Multi output classifier that use gaussian naive bayes estimator \n",
    "clf  = MultiOutputClassifier(GaussianNB(), n_jobs= -1)\n",
    "\n",
    "# perform cross validation on the features and targets using defined classifier\n",
    "scores = cross_val_score(clf, X, y, cv= 5, scoring='f1_weighted', n_jobs=-1)\n",
    "\n",
    "# get cross validation mean score\n",
    "f1_score = np.mean(scores) \n",
    "\n",
    "# show cross validation mean score \n",
    "print('Cross validation mean score is:', f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot durations before outliers removal \n",
    "plt.scatter(df_unsupervised['Duration'], np.ones(df_unsupervised.iloc[:,1].shape))\n",
    "plt.xlabel('Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get quartiles for Duration\n",
    "Q1 = df_unsupervised['Duration'].quantile(0.25)\n",
    "Q3 = df_unsupervised['Duration'].quantile(0.75)\n",
    "\n",
    "# get interquartile range\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove durations that are less than Q1 - 3.5*IQR and durations bigger than  Q3 + 3.5*IQR\n",
    "df_unsupervised = df_unsupervised[~((df_unsupervised < (Q1 - 3.5 * IQR)) |(df_unsupervised > (Q3 + 3.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# for debugging purposes\n",
    "df_unsupervised.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot durations after outliers removal \n",
    "plt.scatter(df_unsupervised['Duration'], np.ones(df_unsupervised.iloc[:,1].shape))\n",
    "plt.xlabel('Duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow Method to determine best number of cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ssd_to_center list(sum of squared distances to closest cluster center)\n",
    "ssd_to_center = []\n",
    "\n",
    "# define k values range that we want to try \n",
    "k_range = range(1,15)\n",
    "\n",
    "for k in tqdm(k_range):\n",
    "    kmeans = KMeans(n_clusters=k) # get an intasnce of kmeans with k = current k (change each iteration)\n",
    "    kmeans.fit(df_unsupervised) # clustering\n",
    "    ssd_to_center.append(kmeans.inertia_) # add sum of squared distances to sum of squared distances list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot elbow method graph\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "plt.plot(k_range, ssd_to_center)\n",
    "plt.scatter(k_range, ssd_to_center, c='r', marker='o')\n",
    "plt.xlabel('k', fontsize=16)\n",
    "plt.ylabel('Sum of Squared Distances', fontsize=16)\n",
    "plt.title('Elbow Method', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4) # get an instance of kmeans with  k = optimal observed k = 4\n",
    "\n",
    "kmeans.fit(df_unsupervised) # clustering \n",
    "\n",
    "labels = kmeans.labels_ # get generated labels\n",
    "\n",
    "centers = kmeans.cluster_centers_ # get clusters centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data clusters and cluster centers plot\n",
    "plt.scatter(df_unsupervised['Duration'],df_unsupervised['Rate'], c=labels)\n",
    "plt.scatter(centers[:,1],centers[:,2],marker='*',c='r',s=300)\n",
    "plt.title('Clusters Visualization')\n",
    "plt.xlabel('Duration')\n",
    "plt.ylabel('Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting frequent itemsets \n",
    "freq_itemsets=apriori(genres, min_support = 0.002,use_colnames=True)\n",
    "\n",
    "# Getting Association Rules\n",
    "rules_list = association_rules(freq_itemsets, metric =\"lift\", min_threshold = 4) \n",
    "\n",
    "# Sorting Association Rules\n",
    "rules_list = rules_list.sort_values(['confidence', 'lift'], ascending =[False, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only rules that have confidence higher than 0.9\n",
    "filtered_rules = rules_list[(rules_list['confidence'] > 0.9)]\n",
    "\n",
    "# show filtered rules\n",
    "filtered_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
